{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "source": [
        "# Simple Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erem1dIrKxK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZiiKW81yEWW",
        "colab_type": "code",
        "outputId": "5c8524d4-dbdc-4706-bf10-2bfdd95f4aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Download 3 datasets.\n",
        "downloaded = drive.CreateFile({'id': '1LPATP-3u2SFafaGuJUq0xV88Apa4UCG1'})\n",
        "downloaded.GetContentFile('qna_chitchat_the_comic.tsv')  \n",
        "print('Downloaded dataset 1')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1NvenGa9eBQ_7N6t0u93315Xygigw_M8f'})\n",
        "downloaded.GetContentFile('qna_chitchat_the_professional.tsv')  \n",
        "print('Downloaded dataset 2')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1tAC1k0Rj6a24V7h6QBdxGtBsyyTafRIj'})\n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv')  \n",
        "print('Downloaded dataset 3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded dataset 1\n",
            "Downloaded dataset 2\n",
            "Downloaded dataset 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "### 1) Decapitalisation\n",
        "- Turn all the charactors into lowercase can reduce the vocabulary size. It also make the following preprocess easier by using one lowercase representation for the same words.\n",
        "\n",
        "### 2) Expand common contractions\n",
        "- Some of the samples in the training datasets have contractions while some are not. Expanding commnon contractions could reduce the variation of the representation of the same meaning which could be a benefitial preprocessing technique especially in these small datasets.\n",
        "\n",
        "### 3) Remove punctuations \n",
        "- Majority of the samples in the datasets are short question answering, therefore,  '?' or '.' is not that different across different data points. \n",
        "- There are not many punctuations in the training datasets. In order to maintain good performance in the acutal case which users might type lots of unseen punctuations, it's better to remove puncuations in the preprocessing before take the data into the model.\n",
        "- Remove digit numbers because it is simple chatbot which is not capable to do conversation with different digits.\n",
        "\n",
        "### 4) Tokenization \n",
        "- Tensorflow Keras tokenization library is used in seq2seq model dataset preprocessing. The tokenizer is fitted in word embedding process which one to one mapping with the word embedding vectors. It could directly transform the sentences into corresponding numerical representations in the seq2seq model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Read datasets\n",
        "comic_dataset = pd.read_csv(\"qna_chitchat_the_comic.tsv\", sep=\"\\t\")\n",
        "professional_dataset = pd.read_csv(\"qna_chitchat_the_professional.tsv\",sep=\"\\t\")\n",
        "friend_dataset = pd.read_csv(\"qna_chitchat_the_friend.tsv\",sep=\"\\t\")\n",
        "\n",
        "# common English contractions dict data is from Lab05 materials.\n",
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "# Expand common English contractions\n",
        "def expand_contractions(text):\n",
        "  \n",
        "    compiled = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    def replace(contraction):\n",
        "        return contraction_dict[contraction.group(0)]\n",
        "    expanded = compiled.sub(replace, text)\n",
        "    \n",
        "    return expanded\n",
        "\n",
        "# Clean the data\n",
        "def cleaning_data(text):\n",
        "    text = text.lower() # decapitalisation\n",
        "    text = expand_contractions(text) # expand contractions\n",
        "    text = re.sub(r'[^\\w\\s]','', text) # remove punctuations\n",
        "    text = re.sub('\\d', '', text) # remove digits\n",
        "    \n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOvqcd-XgYR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Answer data preparation: generate answer dictionary\n",
        "'''\n",
        "def answer_preparation(answers):\n",
        "    # Extract unique answer value\n",
        "    unique_answer = list(set(answers))  \n",
        "    # Add special tokens in the unique answer list\n",
        "    unique_answer.append('_B_')\n",
        "    unique_answer.append('_E_')\n",
        "    unique_answer.append('_U_')\n",
        "    # Generate dictionary for answer data.\n",
        "    answer_word2index = {word: index for index, word in enumerate(unique_answer)}\n",
        "    answer_index2word = {index: word for index, word in enumerate(unique_answer)}\n",
        "    \n",
        "    return answer_word2index, answer_index2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlBVUwAMsEjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "Preprocess dataset before training the seq2seq model\n",
        "'''\n",
        "def Preprocess_Data(dataset):\n",
        "    questions, answers = list(), list()\n",
        "    \n",
        "    for index, row in dataset.iterrows():   \n",
        "        questions.append(cleaning_data(row.Question)) # clean the data\n",
        "        answers.append(row.Answer)\n",
        "    \n",
        "    # generate answer dictionary for decoding\n",
        "    answer_word2index, answer_index2word = answer_preparation(answers)\n",
        "    \n",
        "    # return training data\n",
        "    train_data = {'questions':questions, \n",
        "                  'answers':answers, \n",
        "                  'answer_word2index':answer_word2index, \n",
        "                  'answer_index2word':answer_index2word\n",
        "                 }       \n",
        "    return train_data \n",
        "\n",
        "'''\n",
        "Preprocess each dataset\n",
        "'''\n",
        "comic = Preprocess_Data(comic_dataset)\n",
        "professional = Preprocess_Data(professional_dataset)\n",
        "friend = Preprocess_Data(friend_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "source": [
        "####  Word2Vec with SkipGram is implemented as a Word Embedding model.\n",
        "- As shown the Graph below, the chatbot training datasets consist of short sentences with relatively simple word, therefore, word level of embeddings is enough for this task. It is not necessary to use subwords.\n",
        "- Majority of sentences have 2 to 4 words in the training datasets, the SkipGram with window sides 2 is appropriate for training the word embeddings and used in chatbot training later on.\n",
        "- For the Answer sentences in the chat datasets are not used in word embedding training. The answers will use one hot word vector instead of word embeddings because they are sentence level and vocabularies of sentence are limited in this case, thus it is not necessary to generate sentence level embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HinI_kagYWE",
        "colab_type": "code",
        "outputId": "275619f4-6b27-4375-c185-5d7802663749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Combine three personality chat datasets\n",
        "chat_dataset = pd.concat([comic_dataset, professional_dataset, friend_dataset])\n",
        "\n",
        "# Calculate length of each sentences in chat_dataset.Question\n",
        "sentence_len = [len(sentence.split()) for sentence in chat_dataset.Question]\n",
        "\n",
        "# Distribution of the length of each sentences in the training datasets.\n",
        "sentence_len_df = pd.DataFrame(sentence_len)\n",
        "print(sentence_len_df.describe())\n",
        "\n",
        "# plot length distribution\n",
        "sentence_len_df.plot.hist(bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 0\n",
            "count  1971.000000\n",
            "mean      3.786910\n",
            "std       1.600524\n",
            "min       1.000000\n",
            "25%       3.000000\n",
            "50%       4.000000\n",
            "75%       5.000000\n",
            "max      11.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6I1_K7HTub",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Op66omXKVHa",
        "colab_type": "text"
      },
      "source": [
        "'Microsoft BotBuilder chat datasets' Questions and 'Cornell Movie Dialog Corpus' dataset are used for training Word Embeddings.\n",
        "- The movie plot dataset is used additionally because the chatbot datasets are relatively small and vocabulary is limited (as shown in 1.2.1, the vocabulary size is 506 in chat_dataset). \n",
        "- The whole movie dialog datasets contain nearly 30 thousands lines, which takes too much time to train the model, therefore, half of the datasets(15,000) are selected as the training data in the word embeddings model. Together with the chat datasets, final datasets have 16,971 sentences. \n",
        "\n",
        "*Benefits*:\n",
        "\n",
        "- Enlarge vocabulary size which could reduce the number of unknown words when in the real test cases.\n",
        "- Achieve better Word Embedding vectors by training with larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FwMzcG8lQzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Download movie dialog datasets for word embedding\n",
        "downloaded = drive.CreateFile({'id':'1ah9IvYyW5KxcRlYaYTDMvdYzK7hvAgbE'}) \n",
        "downloaded.GetContentFile(\"movie_lines.txt\")  \n",
        "\n",
        "#2. Use three personality datasets above together with movie dialog datasets."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "source": [
        " - Same as the Personality chat datasets preprocessing, decapitalisation, expanding common contractions and removing punctuations techniques are used in Data Preprocessing for Word Embeddings because there is no any punctuation, contraction or upper case letter in the traning and real test process, therefore, it is not necessory to add them into word embeddings.\n",
        " \n",
        "- Additionally, null values are removed in the first data cleaning step for movie plot data.\n",
        "- Keras tokenizer is used in tokenize the training datasets. It not only tokenize the sequence into individual words efficiently but also turn all the vocabularies into unique numerical representations which would be used in the training processs later on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kELNNY45G3ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine movie dialog datasets and three chat datasets.\n",
        "all_data =[]\n",
        "\n",
        "# Read movie dialog dataset\n",
        "count=0\n",
        "with open('movie_lines.txt', 'r', errors='ignore') as f:\n",
        "    for line in f:\n",
        "        # extract sentences\n",
        "        l = line.strip().split('+++$+++') \n",
        "        if len(l) == 5:\n",
        "            all_data.append(cleaning_data(l[4])) \n",
        "            \n",
        "            count +=1\n",
        "        if count >=15000: # only extract half of the datapoints 15000 out of 30000\n",
        "            break\n",
        "            \n",
        "# Add chat datasets into all_data            \n",
        "for sentence in chat_dataset.Question:\n",
        "    all_data.append(cleaning_data(sentence))               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg7KTX7BKJI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer() \n",
        "\n",
        "# tokenizer fits on all training data\n",
        "tokenizer.fit_on_texts(all_data) \n",
        "\n",
        "# tokenize the training data \n",
        "all_data_tokenized = tokenizer.texts_to_sequences(all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH7H0iu6KVbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Generate Word Embedding training dataset\n",
        "'''\n",
        "input_X, label_Y = list(), list()\n",
        "\n",
        "for sentence in all_data_tokenized:\n",
        "  \n",
        "    if len(sentence) ==1: # for the sentences only contain one words.\n",
        "        input_X.append(sentence[0])\n",
        "        label_Y.append(sentence[0])\n",
        "\n",
        "    else:\n",
        "        for i in range(1,len(sentence)-2):\n",
        "            # second word as a target word\n",
        "            input_X.append(sentence[i])\n",
        "            label_Y.append(sentence[i-1])\n",
        "\n",
        "            \n",
        "            if i>1: # from third word as a target word\n",
        "                input_X.append(sentence[i])\n",
        "                label_Y.append(sentence[i-2])\n",
        "                \n",
        "            if i+1< len(sentence): # check if reach at the end of the sentence\n",
        "                input_X.append(sentence[i])\n",
        "                label_Y.append(sentence[i+1])\n",
        "                \n",
        "            if i+2 < len(sentence): # check if reach at the end of the sentence\n",
        "                input_X.append(sentence[i])\n",
        "                label_Y.append(sentence[i+2])               \n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtSozZTsDX_u",
        "colab": {}
      },
      "source": [
        "# Define a function to generate batch inputs\n",
        "def prepare_batch(input_X, label_Y, batch_size):\n",
        "    \n",
        "    batch_inputs = []\n",
        "    \n",
        "    for batch_i in range(0, len(input_X)//batch_size): \n",
        "        start_i = batch_i*batch_size\n",
        "        batch_input = np.array(input_X[start_i:start_i + batch_size])\n",
        "        batch_label = np.array(label_Y[start_i:start_i + batch_size]).reshape(batch_size, 1)        \n",
        "        batch_inputs.append((batch_input,batch_label))\n",
        "   \n",
        "    return batch_inputs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "source": [
        "**Embedding size: 200**\n",
        "- Choose bigger embedding size to better represent the word. However, bigger embedding size is computationally expensive and also not necessary for small vocabulary. In this case, for total vacabulary size =11,314 and training dataset size =495,668 , embedding size  200 performs great in terms of result accuracy and computational efficiency.\n",
        "\n",
        "**Learning rate: 0.008**\n",
        "**Epoch: 18**\n",
        "\n",
        "- Figure Below is Visualisation of epoch-cost graph during the Word Embeddings model training process on different learning rates.\n",
        "As shown in the figure, learning rate 0.008 and epoch 18 is the optimal choice of hyperparameters.\n",
        "- Compared on same condition: embedding_size = 200, batch_size = 256, epoch = 20\n",
        "- learning rate = [0.005, 0.008, 0.01, 0.05]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC8Mu6gEpVgs",
        "colab_type": "code",
        "outputId": "fa2116de-4438-4287-c414-00a6232b13ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# plot epoch-cost in different learning rate settings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epoch_20 = [i+1 for i in range(20)]\n",
        "\n",
        "# learning_rate = 0.05\n",
        "cost_005 = [14.606675, 9.983141, 8.957794, 11.007188, 12.1262245, 13.044088, 15.638842, 15.686038, 13.912088, 16.034172, 14.697906, 12.672653, 17.132229, 13.085133, 13.486287, 14.541117, 16.869387, 22.326859, 12.300587, 13.172368]\n",
        "\n",
        "# learning_rate = 0.01\n",
        "cost_001 = [5.501668, 4.833394, 4.4415703, 4.2230067, 4.0181265, 4.4546504, 4.0039854, 4.0096474, 4.1387367, 4.5669823, 4.1412916, 4.301157, 3.8959527, 4.1269712, 3.796802, 3.814628, 3.9161959, 4.169813, 3.9905632, 4.353211]\n",
        "\n",
        "# learning_rate = 0.008\n",
        "cost_0008 = [7.6957035, 4.770343, 4.3139243, 4.1474323, 4.061558, 4.8282847, 4.07117, 4.5168314, 4.3105745, 3.8014207, 4.330092, 3.8882437, 3.8810558, 4.0024176, 3.8996267, 3.90563, 3.8651938, 3.91022, 4.4067707, 4.285951]\n",
        "\n",
        "# learning_rate = 0.005\n",
        "cost_0005 = [11.251139, 5.51412, 4.6841617, 4.5836244, 4.2326894, 4.005059, 3.9186711, 3.8187976, 3.8207495, 3.6481996, 4.0861535, 3.8690357, 3.7734966, 3.6502302, 3.8161292, 3.7751307, 4.4331813, 4.2135196, 3.9421976, 4.3114514]\n",
        "\n",
        "plt.plot(epoch_20, cost_005, label='cost=0.05')\n",
        "plt.plot(epoch_20, cost_001, label='cost=0.01')\n",
        "plt.plot(epoch_20, cost_0008, label='cost=0.008')\n",
        "plt.plot(epoch_20, cost_0005, label='cost=0.005')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Cost on different learning rate'); \n",
        "plt.xlabel('epoch'); \n",
        "plt.ylabel('cost')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3vnXlTnDVmE",
        "colab": {}
      },
      "source": [
        "# Build word embedding model\n",
        "import tensorflow as tf\n",
        "\n",
        "embedding_size = 200 \n",
        "batch_size = 256 \n",
        "learning_rate = 0.008\n",
        "\n",
        "sample_size = 128 # sampling size for loss function\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "inputs = tf.placeholder(tf.int32, shape=[batch_size], name='inputs')\n",
        "labels = tf.placeholder(tf.int32, shape=[batch_size, 1], name='labels')\n",
        "\n",
        "# word2vec Model\n",
        "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
        "\n",
        "embedding_vectors = tf.nn.embedding_lookup(embeddings, inputs, name='embedding_vectors')\n",
        "\n",
        "# weight and bias for nce_loss() function\n",
        "weights = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name='weights')\n",
        "biases = tf.Variable(tf.zeros([vocabulary_size]), name='biases')\n",
        "\n",
        "cost_op = tf.reduce_mean(tf.nn.nce_loss(\n",
        "    weights, biases, labels, embedding_vectors, sample_size, vocabulary_size))\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seA7Rw9FXFqd",
        "colab_type": "code",
        "outputId": "5fa2c679-455d-4245-f705-58f43fa6eac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "'''Training Word Embeddings Model'''\n",
        "epochs = 18\n",
        "display_interval = 1\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "# epoch_graph, cost_graph = [], [] #used in justification visualization.\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    for (input_batch, label_batch) in prepare_batch(input_X, label_Y, batch_size): \n",
        "\n",
        "        sess.run(train_op, feed_dict={inputs:input_batch, labels:label_batch})    \n",
        "\n",
        "    if epoch % display_interval == 0 :\n",
        "        # calculate the cost of the current model\n",
        "        cost = sess.run(cost_op, feed_dict={inputs:input_batch, labels:label_batch})\n",
        "        print(\"Epoch \" + str(epoch) + \", Cost= \" + \"{:.4f}\".format(cost))\n",
        "        \n",
        "        # save epoch and cost for visualisation, used in justification visualization.\n",
        "#         epoch_graph.append(epoch)\n",
        "#         cost_graph.append(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Cost= 6.4489\n",
            "Epoch 1, Cost= 5.5970\n",
            "Epoch 2, Cost= 4.2305\n",
            "Epoch 3, Cost= 4.9951\n",
            "Epoch 4, Cost= 4.5621\n",
            "Epoch 5, Cost= 3.7645\n",
            "Epoch 6, Cost= 4.6755\n",
            "Epoch 7, Cost= 4.1537\n",
            "Epoch 8, Cost= 4.3851\n",
            "Epoch 9, Cost= 4.6676\n",
            "Epoch 10, Cost= 4.5507\n",
            "Epoch 11, Cost= 3.6952\n",
            "Epoch 12, Cost= 4.3434\n",
            "Epoch 13, Cost= 4.0330\n",
            "Epoch 14, Cost= 4.2604\n",
            "Epoch 15, Cost= 3.9658\n",
            "Epoch 16, Cost= 3.9862\n",
            "Epoch 17, Cost= 4.5958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccvGXKwOwexe",
        "colab_type": "code",
        "outputId": "8c6bad1f-4d6a-4995-ca23-b8fa9a5172fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# learned embeddings\n",
        "with sess.as_default():\n",
        "    embedding_matrix = embeddings.eval()\n",
        "embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.72747207, -0.57962656,  0.89630485, ..., -0.01899862,\n",
              "         0.19822645,  0.41107512],\n",
              "       [ 0.26535383,  0.1029154 ,  0.1448514 , ...,  0.20774543,\n",
              "        -0.38749447, -0.06851911],\n",
              "       [ 0.3053288 ,  0.08841152,  0.3292596 , ...,  0.33441165,\n",
              "        -0.27151206, -0.02670388],\n",
              "       ...,\n",
              "       [-0.9379034 ,  0.83551145, -0.4309504 , ...,  0.64013076,\n",
              "         0.8058648 ,  0.1853602 ],\n",
              "       [ 0.82260543,  2.362698  , -0.26143828, ..., -0.48504528,\n",
              "        -0.05656002, -0.40946335],\n",
              "       [ 1.5368797 ,  0.28561488,  0.719077  , ..., -0.70566887,\n",
              "         1.457389  , -1.0534751 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwicNPkIqd1",
        "colab_type": "code",
        "outputId": "f86970b4-c826-41c3-8243-a3aabe0c0df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "'''\n",
        "Save the model in the colab local server\n",
        "'''\n",
        "# Save the word embeddings model\n",
        "saver = tf.train.Saver([embeddings])\n",
        "save_path = saver.save(sess, './embedding_model/embeddings')\n",
        "print(\"Word Embeddings Model saved in path: %s\" % save_path)\n",
        "\n",
        "# Save tokenizer, answer dictionary.\n",
        "import pickle\n",
        "pickle.dump(tokenizer, open('./embedding_model/tokenizer.pkl','wb'))\n",
        "print(\"Question tokenizer is saved successfully.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Embeddings Model saved in path: ./embedding_model/embeddings\n",
            "Question tokenizer is saved successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeaQKAmRipM9",
        "colab_type": "code",
        "outputId": "cc852f37-cbcd-4a0d-f01b-6b1e99402403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "'''\n",
        "Upload the Word Embeddings Model in the colab local server to Google Drive\n",
        "'''\n",
        "import os\n",
        "directory = \"/content/embedding_model/\"\n",
        "filelist = os.listdir(directory) # read files in the directory\n",
        "\n",
        "for file_name in filelist:\n",
        "    # Upload model files\n",
        "    try: \n",
        "        uploaded = drive.CreateFile()\n",
        "        uploaded.SetContentFile(file_name)\n",
        "        uploaded.Upload()\n",
        "        \n",
        "        print('{} uploaded with ID {}.\\n'.format(file_name, uploaded.get('id')))\n",
        "    except Exception:\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint uploaded with ID 1TfBc1vVYI40jvU7poPTXPJ8AnaOpoJ49.\n",
            "\n",
            "embeddings.meta uploaded with ID 11qhzF4RmTzJaJjAHhUGK9yc4jpYLJtdk.\n",
            "\n",
            "embeddings.index uploaded with ID 1aTmFyFjPXginLO2COVdj3_tuO2HyCFZT.\n",
            "\n",
            "embeddings.data-00000-of-00001 uploaded with ID 1f-TJXVVEIkttDZCIIhNLB0sLnp125O5k.\n",
            "\n",
            "\n",
            "tokenizer.pkl uploaded with ID 1wfH4pdilN4GvnwH_lwoPXiGQhO1H95UB.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Load_Word_Embeddings_Model():\n",
        "    print('Downloading word embeddings model...')\n",
        "    # Download the Word Embeddings Model from Google Drive\n",
        "    downloaded = drive.CreateFile({'id': '11qhzF4RmTzJaJjAHhUGK9yc4jpYLJtdk'})\n",
        "    downloaded.GetContentFile('embeddings.meta')  \n",
        "\n",
        "    downloaded = drive.CreateFile({'id': '1aTmFyFjPXginLO2COVdj3_tuO2HyCFZT'})\n",
        "    downloaded.GetContentFile('embeddings.index')    \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '1f-TJXVVEIkttDZCIIhNLB0sLnp125O5k'})\n",
        "    downloaded.GetContentFile('embeddings.data-00000-of-00001') \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '1TfBc1vVYI40jvU7poPTXPJ8AnaOpoJ49'})\n",
        "    downloaded.GetContentFile('checkpoint') \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '1wfH4pdilN4GvnwH_lwoPXiGQhO1H95UB'})\n",
        "    downloaded.GetContentFile('tokenizer.pkl') \n",
        "    \n",
        "    print('Word embeddings model is successfully downloaded.')\n",
        "    \n",
        "    # Load the model to colab local server\n",
        "    print('Loading the model to local server...')\n",
        "    saver = tf.train.import_meta_graph('./embeddings.meta')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, \"./embeddings\")\n",
        "        graph = tf.get_default_graph()\n",
        "        embeddings = graph.get_tensor_by_name(\"embeddings:0\")\n",
        "        embedding_matrix = embeddings.eval()\n",
        "\n",
        "    tokenizer = pickle.load(open(\"./tokenizer.pkl\",\"rb\"))\n",
        "    \n",
        "    print('Word Embeddings Model is successfully loaded.')  \n",
        "    \n",
        "    return embedding_matrix, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZBGGw3ekPAd",
        "colab_type": "code",
        "outputId": "36d0cf1a-8436-4cb6-8035-3152e649b879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle    \n",
        "embedding_matrix, tokenizer = Load_Word_Embeddings_Model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading word embeddings model...\n",
            "Word embeddings model is successfully downloaded.\n",
            "Loading the model to local server...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./embeddings\n",
            "Word Embeddings Model is successfully loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1QkCRn-edOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Learned embeddings imported as 'embedding_matrix' in the loading Word Embeddings Model section.\n",
        "The tokenizer fitted in word embedding model training process, imported as 'tokenizer'\n",
        "'''\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "# Define a function of tokenization and padding for input data.\n",
        "def tokenization(sentences):\n",
        "  \n",
        "    # Tokenize all sentences using tokenizer from previous word embeddings model\n",
        "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "    \n",
        "    # calculate max length of sentence\n",
        "    maxlen = max([len(sentence) for sentence in tokenized_sentences])\n",
        "\n",
        "    # padding the sentences with max length\n",
        "    padded_sentences = pad_sequences(tokenized_sentences, maxlen = maxlen) \n",
        "    \n",
        "    return padded_sentences, maxlen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvkTrkR7geZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Generate encoder input, decoder input and target data for seq2seq model training process.\n",
        "'''\n",
        "import numpy as np \n",
        "\n",
        "def get_train_data(dataset):\n",
        "  \n",
        "    questions = dataset['questions']  \n",
        "    answers = dataset['answers']\n",
        "    answer_word2index = dataset['answer_word2index']\n",
        "    answer_index2word = {index:word for word, index in answer_word2index.items()}\n",
        "    \n",
        "    # generate encoder inputs by tokenization and padding\n",
        "    # output max length of encoder inputs - maxlen      \n",
        "    padded, maxlen = tokenization(questions)\n",
        "    encoder_input_data = padded\n",
        "        \n",
        "    # calculate total number of decoder outputs. \n",
        "    n_class = len(answer_word2index)\n",
        "    \n",
        "    # generate decoder inputs\n",
        "    decoder_input_data, decoder_target_data = list(), list()\n",
        "    \n",
        "    for answer in answers:         \n",
        "        # generate decoder input data \n",
        "        dec_input = [answer_word2index['_B_']]\n",
        "        dec_input.append(answer_word2index[answer])\n",
        "        decoder_input_data.append(np.eye(n_class)[dec_input])\n",
        "         \n",
        "        # generate decoder target data\n",
        "        target = dec_input[1:]\n",
        "        target.append(answer_word2index['_E_']) \n",
        "        decoder_target_data.append(target)\n",
        "\n",
        "    decoder_input_data = np.array(decoder_input_data)\n",
        "    decoder_target_data = np.array(decoder_target_data)\n",
        "    \n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data, answer_word2index, answer_index2word, maxlen, n_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr6czDmS0IYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Build Seq2Seq Model using Tensorflow Keras\n",
        "'''\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, LSTM, Input, Embedding, TimeDistributed,Dropout\n",
        "\n",
        "def build_seq2seq_model(maxlen, n_class):\n",
        "    n_units=64\n",
        "    num_en_words = embedding_matrix.shape[0]\n",
        "    embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    # encoder layers:\n",
        "    encoder_input = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(num_en_words,\n",
        "                        embedding_dim,\n",
        "                        input_length=maxlen,\n",
        "                        weights=[embedding_matrix], trainable=False)(encoder_input)\n",
        "    encoder_LSTM = LSTM(n_units, recurrent_dropout=0.5, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding) \n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # decoder layers\n",
        "    decoder_input = Input(shape=(None, n_class))\n",
        "    decoder_LSTM = LSTM(n_units, recurrent_dropout=0.5, return_sequences=True, return_state=True)\n",
        "    decoder,_,_ = decoder_LSTM(decoder_input, initial_state = encoder_states)\n",
        "    decoder_dense = Dense(n_class, activation='linear', name='decoder_output') \n",
        "    decoder_outputs = decoder_dense(decoder)  \n",
        "    \n",
        "    # Define encoder model\n",
        "    encoder_model = Model(encoder_input, encoder_states)\n",
        "\n",
        "    # Define training model\n",
        "    model = Model([encoder_input, decoder_input], decoder_outputs)\n",
        "\n",
        "    # Define decoder model\n",
        "    dec_h = Input(shape=(n_units,))\n",
        "    dec_c = Input(shape=(n_units,))\n",
        "    dec_states_inputs = [dec_h, dec_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_LSTM(decoder_input, initial_state=dec_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    decoder_model = Model([decoder_input] + dec_states_inputs, [decoder_outputs] + decoder_states)\n",
        "#     model.summary()\n",
        "    \n",
        "    return model, encoder_model, decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3. Train Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fuB5uRGMIVZ",
        "colab_type": "text"
      },
      "source": [
        "Train the model with different batch size, learning rate and epoch. The 'comic' datasets was used in tuning models.\n",
        "The results are shown as below figures.\n",
        "\n",
        "***batch size:*** \n",
        "\n",
        "- Batch size 64, 128, 256 were choen to perform hyperparameter tuning. Based on the result shown in the figure, after enough iteration, loss from different batch sizes are similar. In this case, we will run at leat 200 epochs, it means the batch size is not critical issues. Therefore batch size = 128 is chosen as a default value.\n",
        "\n",
        "***learning rate:*** \n",
        "- As shown in the figure below, the model was trained and validated with learing rate: 0.0005, 0.001, 0.005, 0.01. After training through 300 epoch, learning rate 0.005 and 0.01 are already reached minimum loss from epoch 100 and after epoch 300 they are still keep lowest loss, while learning rate 0.0005 is too slow to reduce the loss. Therefore, learning rate 0.01 was chosen to train final model in terms of effiency.\n",
        "\n",
        "***epoch:*** \n",
        "- After trained with 300 epochs, the model already reached minimun after 100 epochs with learning rate 0.01, Thus, epoch=200 is enough in this task. \n",
        "\n",
        "Final hyperparameter: batch size = 128, learning rate = 0.01 , epoch =200\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7oZ7UZIbBBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Below training function is only for visualize learning cruve with different learning rate.\n",
        "'''\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Generate training data - select comic data as a sample data for hyperparameter tuning.\n",
        "encoder_input_data, decoder_input_data, decoder_target_data,\\\n",
        "        answer_word2index, answer_index2word, \\\n",
        "                maxlen, n_class = get_train_data(comic)\n",
        "# # Build model\n",
        "# model, encoder_model, decoder_model = build_seq2seq_model(maxlen, n_class)\n",
        "\n",
        "# define loss function: use sparse_softmax_cross_entropy_with_logits\n",
        "def sparse_loss(targets, decoder_outputs):\n",
        "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=decoder_outputs)\n",
        "\n",
        "def tune_model(learning_rate=0.005, batch_size = 128, epochs = 300):\n",
        "    \n",
        "    # Build model\n",
        "    model, encoder_model, decoder_model = build_seq2seq_model(maxlen, n_class)\n",
        "    # Run training\n",
        "    decoder_target = tf.placeholder(dtype='int32', shape=(None, n_class))    \n",
        "    model.compile(optimizer=Adam(lr=learning_rate),\n",
        "                    loss=sparse_loss,\n",
        "                    target_tensors=[decoder_target])\n",
        "\n",
        "    model_fit = model.fit(x=[encoder_input_data, decoder_input_data], y=decoder_target_data,\n",
        "                batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "    \n",
        "    loss = model_fit.history['loss']\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG-1imrfbGia",
        "colab_type": "code",
        "outputId": "0e598ea7-8a3c-4cc5-9e07-19f0f208528d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "'''\n",
        "Learning curve with different batch size.\n",
        "'''\n",
        "batch_size_list = [64,128,256] \n",
        "\n",
        "for batch_size in batch_size_list:\n",
        "    print('training with batch_size = {}'.format(batch_size))\n",
        "    loss = tune_model(learning_rate=0.005, batch_size = batch_size, epochs = 200)\n",
        "    epoch = [i for i in range(200)]\n",
        "    plt.plot(epoch, loss, label=str(batch_size))\n",
        "    \n",
        "plt.legend()\n",
        "plt.title('different batch size');\n",
        "plt.xlabel('epoch'); \n",
        "plt.ylabel('loss')\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training with batch_size = 64\n",
            "training with batch_size = 128\n",
            "training with batch_size = 256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caYBdtn4CjCt",
        "colab_type": "code",
        "outputId": "775e9856-ba44-45c5-e470-8ff00f5433ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "'''\n",
        "Learning curve with different learning rate.\n",
        "'''\n",
        "learning_rate_list = [0.0005, 0.001, 0.005, 0.01] \n",
        "\n",
        "for learning_rate in learning_rate_list:\n",
        "    print('training with learning_rate = {}'.format(learning_rate))\n",
        "    loss = tune_model(learning_rate=learning_rate, epochs = 300)\n",
        "    epoch = [i for i in range(300)]\n",
        "    plt.plot(epoch, loss, label=str(learning_rate))\n",
        "\n",
        "plt.legend()\n",
        "plt.title('different learning rate');\n",
        "plt.xlabel('epoch'); \n",
        "plt.ylabel('loss')\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training with learning_rate = 0.0005\n",
            "training with learning_rate = 0.001\n",
            "training with learning_rate = 0.005\n",
            "training with learning_rate = 0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukJuEOCWNswp",
        "colab_type": "text"
      },
      "source": [
        "***Below is actual training model***\n",
        "\n",
        "batch size = 128, learning rate = 0.01 , epoch =200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gpCNo2GMGEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# define loss function: use sparse_softmax_cross_entropy_with_logits\n",
        "def sparse_loss(targets, decoder_outputs):\n",
        "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=decoder_outputs)\n",
        "\n",
        "def Train_Seq2Seq_Model(dataset, learning_rate=0.01, batch_size = 128, epochs = 200):\n",
        "    \n",
        "    # Generate training data\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, \\\n",
        "            answer_word2index, answer_index2word, \\\n",
        "                    maxlen, n_class = get_train_data(dataset)      \n",
        "    print('maxlen: {}, n_class: {} \\n'.format(maxlen, n_class))\n",
        "    \n",
        "    # Build model\n",
        "    model, encoder_model, decoder_model = build_seq2seq_model(maxlen, n_class)\n",
        "    model.summary()\n",
        "    \n",
        "    # Run training\n",
        "    decoder_target = tf.placeholder(dtype='int32', shape=(None, n_class))    \n",
        "    model.compile(optimizer=Adam(lr=learning_rate),\n",
        "                    loss=sparse_loss,\n",
        "                    target_tensors=[decoder_target])\n",
        "\n",
        "    model.fit(x = [encoder_input_data, decoder_input_data], y = decoder_target_data,\n",
        "          batch_size=batch_size, epochs=epochs) \n",
        "    \n",
        "    return encoder_model, decoder_model, answer_word2index, answer_index2word, maxlen, n_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzwZFvh3qvcf",
        "colab_type": "text"
      },
      "source": [
        "**Train three personality chat models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_4GOuzzsn7m",
        "colab_type": "code",
        "outputId": "0f72c977-5980-417f-a140-6e82d092d387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7245
        }
      },
      "source": [
        "# train the model\n",
        "print(\"-----------start to train Professional chatbot------------\")\n",
        "pro_model = Train_Seq2Seq_Model(professional)\n",
        "\n",
        "# unpack model to save seperately later.\n",
        "pro_enc_model, pro_dec_model, pro_answer_word2index, pro_answer_index2word, maxlen, n_class = pro_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------start to train Professional chatbot------------\n",
            "maxlen: 11, n_class: 100 \n",
            "\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_81 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_21 (Embedding)        (None, 11, 200)      2263000     input_81[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_82 (InputLayer)           (None, None, 100)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_41 (LSTM)                  [(None, 11, 64), (No 67840       embedding_21[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_42 (LSTM)                  [(None, None, 64), ( 42240       input_82[0][0]                   \n",
            "                                                                 lstm_41[0][1]                    \n",
            "                                                                 lstm_41[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output (Dense)          (None, None, 100)    6500        lstm_42[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,379,580\n",
            "Trainable params: 116,580\n",
            "Non-trainable params: 2,263,000\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "657/657 [==============================] - 12s 18ms/step - loss: 3.6210\n",
            "Epoch 2/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 2.7376\n",
            "Epoch 3/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 2.5096\n",
            "Epoch 4/200\n",
            "657/657 [==============================] - 0s 212us/step - loss: 2.3806\n",
            "Epoch 5/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 2.2797\n",
            "Epoch 6/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 2.2037\n",
            "Epoch 7/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 2.1324\n",
            "Epoch 8/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 2.0466\n",
            "Epoch 9/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 1.9614\n",
            "Epoch 10/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 1.8789\n",
            "Epoch 11/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 1.7847\n",
            "Epoch 12/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 1.7058\n",
            "Epoch 13/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 1.5998\n",
            "Epoch 14/200\n",
            "657/657 [==============================] - 0s 228us/step - loss: 1.5083\n",
            "Epoch 15/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 1.4207\n",
            "Epoch 16/200\n",
            "657/657 [==============================] - 0s 228us/step - loss: 1.3296\n",
            "Epoch 17/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 1.2384\n",
            "Epoch 18/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 1.1431\n",
            "Epoch 19/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 1.0443\n",
            "Epoch 20/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.9746\n",
            "Epoch 21/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.8926\n",
            "Epoch 22/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.8246\n",
            "Epoch 23/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.7618\n",
            "Epoch 24/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.7078\n",
            "Epoch 25/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.6705\n",
            "Epoch 26/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.6220\n",
            "Epoch 27/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.5569\n",
            "Epoch 28/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.5187\n",
            "Epoch 29/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.4884\n",
            "Epoch 30/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.4346\n",
            "Epoch 31/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.4025\n",
            "Epoch 32/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.3642\n",
            "Epoch 33/200\n",
            "657/657 [==============================] - 0s 198us/step - loss: 0.3445\n",
            "Epoch 34/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.3265\n",
            "Epoch 35/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.2953\n",
            "Epoch 36/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.2620\n",
            "Epoch 37/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.2546\n",
            "Epoch 38/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.2316\n",
            "Epoch 39/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.2181\n",
            "Epoch 40/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.2137\n",
            "Epoch 41/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.1893\n",
            "Epoch 42/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.1797\n",
            "Epoch 43/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.1645\n",
            "Epoch 44/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.1513\n",
            "Epoch 45/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.1458\n",
            "Epoch 46/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.1351\n",
            "Epoch 47/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.1289\n",
            "Epoch 48/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.1208\n",
            "Epoch 49/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.1155\n",
            "Epoch 50/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.1035\n",
            "Epoch 51/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0989\n",
            "Epoch 52/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.1016\n",
            "Epoch 53/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0929\n",
            "Epoch 54/200\n",
            "657/657 [==============================] - 0s 227us/step - loss: 0.0898\n",
            "Epoch 55/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0847\n",
            "Epoch 56/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0754\n",
            "Epoch 57/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0711\n",
            "Epoch 58/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0674\n",
            "Epoch 59/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0650\n",
            "Epoch 60/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0626\n",
            "Epoch 61/200\n",
            "657/657 [==============================] - 0s 223us/step - loss: 0.0576\n",
            "Epoch 62/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0558\n",
            "Epoch 63/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0586\n",
            "Epoch 64/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0529\n",
            "Epoch 65/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0540\n",
            "Epoch 66/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0518\n",
            "Epoch 67/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0462\n",
            "Epoch 68/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0442\n",
            "Epoch 69/200\n",
            "657/657 [==============================] - 0s 228us/step - loss: 0.0402\n",
            "Epoch 70/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0430\n",
            "Epoch 71/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0385\n",
            "Epoch 72/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0377\n",
            "Epoch 73/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0359\n",
            "Epoch 74/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0342\n",
            "Epoch 75/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0330\n",
            "Epoch 76/200\n",
            "657/657 [==============================] - 0s 223us/step - loss: 0.0322\n",
            "Epoch 77/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0388\n",
            "Epoch 78/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0362\n",
            "Epoch 79/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0382\n",
            "Epoch 80/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0369\n",
            "Epoch 81/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0307\n",
            "Epoch 82/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0299\n",
            "Epoch 83/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0303\n",
            "Epoch 84/200\n",
            "657/657 [==============================] - 0s 231us/step - loss: 0.0344\n",
            "Epoch 85/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0309\n",
            "Epoch 86/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0268\n",
            "Epoch 87/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0265\n",
            "Epoch 88/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0236\n",
            "Epoch 89/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0253\n",
            "Epoch 90/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0234\n",
            "Epoch 91/200\n",
            "657/657 [==============================] - 0s 250us/step - loss: 0.0228\n",
            "Epoch 92/200\n",
            "657/657 [==============================] - 0s 213us/step - loss: 0.0216\n",
            "Epoch 93/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0196\n",
            "Epoch 94/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0210\n",
            "Epoch 95/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0190\n",
            "Epoch 96/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0182\n",
            "Epoch 97/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0195\n",
            "Epoch 98/200\n",
            "657/657 [==============================] - 0s 220us/step - loss: 0.0196\n",
            "Epoch 99/200\n",
            "657/657 [==============================] - 0s 214us/step - loss: 0.0209\n",
            "Epoch 100/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0219\n",
            "Epoch 101/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0207\n",
            "Epoch 102/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0204\n",
            "Epoch 103/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0229\n",
            "Epoch 104/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0184\n",
            "Epoch 105/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0189\n",
            "Epoch 106/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.0187\n",
            "Epoch 107/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0180\n",
            "Epoch 108/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0156\n",
            "Epoch 109/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0168\n",
            "Epoch 110/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0147\n",
            "Epoch 111/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0150\n",
            "Epoch 112/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0141\n",
            "Epoch 113/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.0138\n",
            "Epoch 114/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0141\n",
            "Epoch 115/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0132\n",
            "Epoch 116/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0136\n",
            "Epoch 117/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0135\n",
            "Epoch 118/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0139\n",
            "Epoch 119/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0140\n",
            "Epoch 120/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0119\n",
            "Epoch 121/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.0133\n",
            "Epoch 122/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0135\n",
            "Epoch 123/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0135\n",
            "Epoch 124/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0124\n",
            "Epoch 125/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0125\n",
            "Epoch 126/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0137\n",
            "Epoch 127/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0153\n",
            "Epoch 128/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0161\n",
            "Epoch 129/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.0165\n",
            "Epoch 130/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0140\n",
            "Epoch 131/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0146\n",
            "Epoch 132/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0135\n",
            "Epoch 133/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0125\n",
            "Epoch 134/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0124\n",
            "Epoch 135/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0101\n",
            "Epoch 136/200\n",
            "657/657 [==============================] - 0s 233us/step - loss: 0.0126\n",
            "Epoch 137/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0115\n",
            "Epoch 138/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0112\n",
            "Epoch 139/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0118\n",
            "Epoch 140/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0108\n",
            "Epoch 141/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0113\n",
            "Epoch 142/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0138\n",
            "Epoch 143/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0167\n",
            "Epoch 144/200\n",
            "657/657 [==============================] - 0s 214us/step - loss: 0.0149\n",
            "Epoch 145/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0125\n",
            "Epoch 146/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0131\n",
            "Epoch 147/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0128\n",
            "Epoch 148/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0124\n",
            "Epoch 149/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0107\n",
            "Epoch 150/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0128\n",
            "Epoch 151/200\n",
            "657/657 [==============================] - 0s 232us/step - loss: 0.0131\n",
            "Epoch 152/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0128\n",
            "Epoch 153/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0123\n",
            "Epoch 154/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0101\n",
            "Epoch 155/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0097\n",
            "Epoch 156/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0088\n",
            "Epoch 157/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0091\n",
            "Epoch 158/200\n",
            "657/657 [==============================] - 0s 213us/step - loss: 0.0095\n",
            "Epoch 159/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0086\n",
            "Epoch 160/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0082\n",
            "Epoch 161/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0081\n",
            "Epoch 162/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0088\n",
            "Epoch 163/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0089\n",
            "Epoch 164/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0084\n",
            "Epoch 165/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0081\n",
            "Epoch 166/200\n",
            "657/657 [==============================] - 0s 228us/step - loss: 0.0094\n",
            "Epoch 167/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0105\n",
            "Epoch 168/200\n",
            "657/657 [==============================] - 0s 232us/step - loss: 0.0087\n",
            "Epoch 169/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0088\n",
            "Epoch 170/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0085\n",
            "Epoch 171/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0077\n",
            "Epoch 172/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0082\n",
            "Epoch 173/200\n",
            "657/657 [==============================] - 0s 227us/step - loss: 0.0076\n",
            "Epoch 174/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0079\n",
            "Epoch 175/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0079\n",
            "Epoch 176/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0076\n",
            "Epoch 177/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0082\n",
            "Epoch 178/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0073\n",
            "Epoch 179/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0078\n",
            "Epoch 180/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0075\n",
            "Epoch 181/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.0088\n",
            "Epoch 182/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 0.0071\n",
            "Epoch 183/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0075\n",
            "Epoch 184/200\n",
            "657/657 [==============================] - 0s 199us/step - loss: 0.0086\n",
            "Epoch 185/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0091\n",
            "Epoch 186/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0102\n",
            "Epoch 187/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0083\n",
            "Epoch 188/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.0113\n",
            "Epoch 189/200\n",
            "657/657 [==============================] - 0s 215us/step - loss: 0.0109\n",
            "Epoch 190/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0106\n",
            "Epoch 191/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0087\n",
            "Epoch 192/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0081\n",
            "Epoch 193/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0076\n",
            "Epoch 194/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0068\n",
            "Epoch 195/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0081\n",
            "Epoch 196/200\n",
            "657/657 [==============================] - 0s 227us/step - loss: 0.0069\n",
            "Epoch 197/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0087\n",
            "Epoch 198/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0073\n",
            "Epoch 199/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0112\n",
            "Epoch 200/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTcmTjFAsnhl",
        "colab_type": "code",
        "outputId": "3cc0e2e1-ccf2-48f9-da49-a8cc22254bec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7245
        }
      },
      "source": [
        "# train model\n",
        "print(\"-----------start to train Comic chatbot------------\")\n",
        "comic_model = Train_Seq2Seq_Model(comic)\n",
        "\n",
        "# unpack model\n",
        "comic_enc_model, comic_dec_model, comic_answer_word2index, comic_answer_index2word, maxlen, n_class = comic_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------start to train Comic chatbot------------\n",
            "maxlen: 11, n_class: 100 \n",
            "\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_85 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_22 (Embedding)        (None, 11, 200)      2263000     input_85[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_86 (InputLayer)           (None, None, 100)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_43 (LSTM)                  [(None, 11, 64), (No 67840       embedding_22[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_44 (LSTM)                  [(None, None, 64), ( 42240       input_86[0][0]                   \n",
            "                                                                 lstm_43[0][1]                    \n",
            "                                                                 lstm_43[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output (Dense)          (None, None, 100)    6500        lstm_44[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,379,580\n",
            "Trainable params: 116,580\n",
            "Non-trainable params: 2,263,000\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "657/657 [==============================] - 12s 18ms/step - loss: 3.6418\n",
            "Epoch 2/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 2.7548\n",
            "Epoch 3/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 2.5359\n",
            "Epoch 4/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 2.3854\n",
            "Epoch 5/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 2.3004\n",
            "Epoch 6/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 2.2332\n",
            "Epoch 7/200\n",
            "657/657 [==============================] - 0s 198us/step - loss: 2.1893\n",
            "Epoch 8/200\n",
            "657/657 [==============================] - 0s 198us/step - loss: 2.1309\n",
            "Epoch 9/200\n",
            "657/657 [==============================] - 0s 199us/step - loss: 2.0861\n",
            "Epoch 10/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 2.0235\n",
            "Epoch 11/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 1.9613\n",
            "Epoch 12/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 1.8841\n",
            "Epoch 13/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 1.8060\n",
            "Epoch 14/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 1.7257\n",
            "Epoch 15/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 1.6457\n",
            "Epoch 16/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 1.5633\n",
            "Epoch 17/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 1.4522\n",
            "Epoch 18/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 1.3553\n",
            "Epoch 19/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 1.2569\n",
            "Epoch 20/200\n",
            "657/657 [==============================] - 0s 226us/step - loss: 1.1754\n",
            "Epoch 21/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 1.0707\n",
            "Epoch 22/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.9974\n",
            "Epoch 23/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.9233\n",
            "Epoch 24/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.8382\n",
            "Epoch 25/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.7731\n",
            "Epoch 26/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.7072\n",
            "Epoch 27/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.6412\n",
            "Epoch 28/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.5778\n",
            "Epoch 29/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.5437\n",
            "Epoch 30/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.4853\n",
            "Epoch 31/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.4566\n",
            "Epoch 32/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.4201\n",
            "Epoch 33/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.3771\n",
            "Epoch 34/200\n",
            "657/657 [==============================] - 0s 212us/step - loss: 0.3481\n",
            "Epoch 35/200\n",
            "657/657 [==============================] - 0s 220us/step - loss: 0.3230\n",
            "Epoch 36/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.3100\n",
            "Epoch 37/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.2829\n",
            "Epoch 38/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.2510\n",
            "Epoch 39/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.2294\n",
            "Epoch 40/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.2179\n",
            "Epoch 41/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.2052\n",
            "Epoch 42/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.1906\n",
            "Epoch 43/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.1727\n",
            "Epoch 44/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.1572\n",
            "Epoch 45/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.1503\n",
            "Epoch 46/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.1382\n",
            "Epoch 47/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.1368\n",
            "Epoch 48/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.1283\n",
            "Epoch 49/200\n",
            "657/657 [==============================] - 0s 215us/step - loss: 0.1250\n",
            "Epoch 50/200\n",
            "657/657 [==============================] - 0s 218us/step - loss: 0.1165\n",
            "Epoch 51/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.1065\n",
            "Epoch 52/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.1027\n",
            "Epoch 53/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0964\n",
            "Epoch 54/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.0910\n",
            "Epoch 55/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0934\n",
            "Epoch 56/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0862\n",
            "Epoch 57/200\n",
            "657/657 [==============================] - 0s 229us/step - loss: 0.0879\n",
            "Epoch 58/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0796\n",
            "Epoch 59/200\n",
            "657/657 [==============================] - 0s 231us/step - loss: 0.0747\n",
            "Epoch 60/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0696\n",
            "Epoch 61/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0641\n",
            "Epoch 62/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0625\n",
            "Epoch 63/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0614\n",
            "Epoch 64/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.0565\n",
            "Epoch 65/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0543\n",
            "Epoch 66/200\n",
            "657/657 [==============================] - 0s 216us/step - loss: 0.0511\n",
            "Epoch 67/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0512\n",
            "Epoch 68/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0448\n",
            "Epoch 69/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0447\n",
            "Epoch 70/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0432\n",
            "Epoch 71/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0431\n",
            "Epoch 72/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.0431\n",
            "Epoch 73/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0430\n",
            "Epoch 74/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0440\n",
            "Epoch 75/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0418\n",
            "Epoch 76/200\n",
            "657/657 [==============================] - 0s 199us/step - loss: 0.0363\n",
            "Epoch 77/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0366\n",
            "Epoch 78/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0350\n",
            "Epoch 79/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.0340\n",
            "Epoch 80/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0299\n",
            "Epoch 81/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0311\n",
            "Epoch 82/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0304\n",
            "Epoch 83/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0330\n",
            "Epoch 84/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0275\n",
            "Epoch 85/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0298\n",
            "Epoch 86/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0269\n",
            "Epoch 87/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.0302\n",
            "Epoch 88/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0285\n",
            "Epoch 89/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0281\n",
            "Epoch 90/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0263\n",
            "Epoch 91/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0266\n",
            "Epoch 92/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0261\n",
            "Epoch 93/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0238\n",
            "Epoch 94/200\n",
            "657/657 [==============================] - 0s 223us/step - loss: 0.0255\n",
            "Epoch 95/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0220\n",
            "Epoch 96/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0200\n",
            "Epoch 97/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0197\n",
            "Epoch 98/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0204\n",
            "Epoch 99/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0193\n",
            "Epoch 100/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0179\n",
            "Epoch 101/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0184\n",
            "Epoch 102/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.0174\n",
            "Epoch 103/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0180\n",
            "Epoch 104/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0208\n",
            "Epoch 105/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0231\n",
            "Epoch 106/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0231\n",
            "Epoch 107/200\n",
            "657/657 [==============================] - 0s 213us/step - loss: 0.0201\n",
            "Epoch 108/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0169\n",
            "Epoch 109/200\n",
            "657/657 [==============================] - 0s 220us/step - loss: 0.0155\n",
            "Epoch 110/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0155\n",
            "Epoch 111/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0162\n",
            "Epoch 112/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0149\n",
            "Epoch 113/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0165\n",
            "Epoch 114/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0151\n",
            "Epoch 115/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0146\n",
            "Epoch 116/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0150\n",
            "Epoch 117/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 0.0149\n",
            "Epoch 118/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0145\n",
            "Epoch 119/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0150\n",
            "Epoch 120/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0170\n",
            "Epoch 121/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0154\n",
            "Epoch 122/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0153\n",
            "Epoch 123/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0164\n",
            "Epoch 124/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.0145\n",
            "Epoch 125/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0141\n",
            "Epoch 126/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0123\n",
            "Epoch 127/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0128\n",
            "Epoch 128/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0123\n",
            "Epoch 129/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0134\n",
            "Epoch 130/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0132\n",
            "Epoch 131/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0121\n",
            "Epoch 132/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.0103\n",
            "Epoch 133/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0134\n",
            "Epoch 134/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0124\n",
            "Epoch 135/200\n",
            "657/657 [==============================] - 0s 226us/step - loss: 0.0117\n",
            "Epoch 136/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0113\n",
            "Epoch 137/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0109\n",
            "Epoch 138/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0116\n",
            "Epoch 139/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.0116\n",
            "Epoch 140/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0115\n",
            "Epoch 141/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0114\n",
            "Epoch 142/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0104\n",
            "Epoch 143/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0099\n",
            "Epoch 144/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0098\n",
            "Epoch 145/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0099\n",
            "Epoch 146/200\n",
            "657/657 [==============================] - 0s 212us/step - loss: 0.0098\n",
            "Epoch 147/200\n",
            "657/657 [==============================] - 0s 217us/step - loss: 0.0102\n",
            "Epoch 148/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0108\n",
            "Epoch 149/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0092\n",
            "Epoch 150/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0102\n",
            "Epoch 151/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0112\n",
            "Epoch 152/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0130\n",
            "Epoch 153/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0113\n",
            "Epoch 154/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.0101\n",
            "Epoch 155/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0098\n",
            "Epoch 156/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0101\n",
            "Epoch 157/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0093\n",
            "Epoch 158/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0102\n",
            "Epoch 159/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0103\n",
            "Epoch 160/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0090\n",
            "Epoch 161/200\n",
            "657/657 [==============================] - 0s 215us/step - loss: 0.0092\n",
            "Epoch 162/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0110\n",
            "Epoch 163/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0097\n",
            "Epoch 164/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0093\n",
            "Epoch 165/200\n",
            "657/657 [==============================] - 0s 216us/step - loss: 0.0084\n",
            "Epoch 166/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0090\n",
            "Epoch 167/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0086\n",
            "Epoch 168/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0073\n",
            "Epoch 169/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 0.0080\n",
            "Epoch 170/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0082\n",
            "Epoch 171/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0083\n",
            "Epoch 172/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0087\n",
            "Epoch 173/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0084\n",
            "Epoch 174/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0090\n",
            "Epoch 175/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0085\n",
            "Epoch 176/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 0.0092\n",
            "Epoch 177/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0093\n",
            "Epoch 178/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0094\n",
            "Epoch 179/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0077\n",
            "Epoch 180/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0090\n",
            "Epoch 181/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0092\n",
            "Epoch 182/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0139\n",
            "Epoch 183/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0117\n",
            "Epoch 184/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 0.0140\n",
            "Epoch 185/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0130\n",
            "Epoch 186/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0094\n",
            "Epoch 187/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0096\n",
            "Epoch 188/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0071\n",
            "Epoch 189/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0076\n",
            "Epoch 190/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0079\n",
            "Epoch 191/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.0085\n",
            "Epoch 192/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0072\n",
            "Epoch 193/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0067\n",
            "Epoch 194/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0073\n",
            "Epoch 195/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0068\n",
            "Epoch 196/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0063\n",
            "Epoch 197/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0070\n",
            "Epoch 198/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.0084\n",
            "Epoch 199/200\n",
            "657/657 [==============================] - 0s 227us/step - loss: 0.0082\n",
            "Epoch 200/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc072nKQN-GN",
        "colab_type": "code",
        "outputId": "57a3e5cc-3a35-4085-f2de-9f95659ccb2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7245
        }
      },
      "source": [
        "# train model\n",
        "print(\"-----------start to train Friend chatbot------------\")\n",
        "friend_model = Train_Seq2Seq_Model(friend)  \n",
        "\n",
        "# unpack model\n",
        "friend_enc_model, friend_dec_model, friend_answer_word2index, friend_answer_index2word, maxlen, n_class = friend_model      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------start to train Friend chatbot------------\n",
            "maxlen: 11, n_class: 101 \n",
            "\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_89 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_23 (Embedding)        (None, 11, 200)      2263000     input_89[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_90 (InputLayer)           (None, None, 101)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_45 (LSTM)                  [(None, 11, 64), (No 67840       embedding_23[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_46 (LSTM)                  [(None, None, 64), ( 42496       input_90[0][0]                   \n",
            "                                                                 lstm_45[0][1]                    \n",
            "                                                                 lstm_45[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output (Dense)          (None, None, 101)    6565        lstm_46[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,379,901\n",
            "Trainable params: 116,901\n",
            "Non-trainable params: 2,263,000\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "657/657 [==============================] - 12s 19ms/step - loss: 3.5719\n",
            "Epoch 2/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 2.7198\n",
            "Epoch 3/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 2.5165\n",
            "Epoch 4/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 2.3967\n",
            "Epoch 5/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 2.3069\n",
            "Epoch 6/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 2.2316\n",
            "Epoch 7/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 2.1712\n",
            "Epoch 8/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 2.1103\n",
            "Epoch 9/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 2.0363\n",
            "Epoch 10/200\n",
            "657/657 [==============================] - 0s 218us/step - loss: 1.9564\n",
            "Epoch 11/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 1.8666\n",
            "Epoch 12/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 1.7643\n",
            "Epoch 13/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 1.6548\n",
            "Epoch 14/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 1.5566\n",
            "Epoch 15/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 1.4500\n",
            "Epoch 16/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 1.3262\n",
            "Epoch 17/200\n",
            "657/657 [==============================] - 0s 213us/step - loss: 1.2309\n",
            "Epoch 18/200\n",
            "657/657 [==============================] - 0s 223us/step - loss: 1.1285\n",
            "Epoch 19/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 1.0255\n",
            "Epoch 20/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.9227\n",
            "Epoch 21/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.8387\n",
            "Epoch 22/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.7787\n",
            "Epoch 23/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.7003\n",
            "Epoch 24/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.6286\n",
            "Epoch 25/200\n",
            "657/657 [==============================] - 0s 223us/step - loss: 0.5859\n",
            "Epoch 26/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.5299\n",
            "Epoch 27/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.4641\n",
            "Epoch 28/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.4269\n",
            "Epoch 29/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.3868\n",
            "Epoch 30/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.3578\n",
            "Epoch 31/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.3156\n",
            "Epoch 32/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.2923\n",
            "Epoch 33/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.2654\n",
            "Epoch 34/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.2438\n",
            "Epoch 35/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.2253\n",
            "Epoch 36/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.2099\n",
            "Epoch 37/200\n",
            "657/657 [==============================] - 0s 218us/step - loss: 0.1881\n",
            "Epoch 38/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.1788\n",
            "Epoch 39/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.1596\n",
            "Epoch 40/200\n",
            "657/657 [==============================] - 0s 223us/step - loss: 0.1538\n",
            "Epoch 41/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.1352\n",
            "Epoch 42/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.1271\n",
            "Epoch 43/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.1202\n",
            "Epoch 44/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.1195\n",
            "Epoch 45/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.1067\n",
            "Epoch 46/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.1095\n",
            "Epoch 47/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0960\n",
            "Epoch 48/200\n",
            "657/657 [==============================] - 0s 237us/step - loss: 0.0927\n",
            "Epoch 49/200\n",
            "657/657 [==============================] - 0s 235us/step - loss: 0.0861\n",
            "Epoch 50/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0825\n",
            "Epoch 51/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0773\n",
            "Epoch 52/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0721\n",
            "Epoch 53/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0687\n",
            "Epoch 54/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0679\n",
            "Epoch 55/200\n",
            "657/657 [==============================] - 0s 227us/step - loss: 0.0666\n",
            "Epoch 56/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0593\n",
            "Epoch 57/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0539\n",
            "Epoch 58/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0527\n",
            "Epoch 59/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0486\n",
            "Epoch 60/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0467\n",
            "Epoch 61/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0453\n",
            "Epoch 62/200\n",
            "657/657 [==============================] - 0s 229us/step - loss: 0.0426\n",
            "Epoch 63/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0408\n",
            "Epoch 64/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0410\n",
            "Epoch 65/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0395\n",
            "Epoch 66/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0398\n",
            "Epoch 67/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0407\n",
            "Epoch 68/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0388\n",
            "Epoch 69/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0371\n",
            "Epoch 70/200\n",
            "657/657 [==============================] - 0s 226us/step - loss: 0.0335\n",
            "Epoch 71/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0328\n",
            "Epoch 72/200\n",
            "657/657 [==============================] - 0s 200us/step - loss: 0.0308\n",
            "Epoch 73/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0305\n",
            "Epoch 74/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0292\n",
            "Epoch 75/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0292\n",
            "Epoch 76/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0290\n",
            "Epoch 77/200\n",
            "657/657 [==============================] - 0s 230us/step - loss: 0.0285\n",
            "Epoch 78/200\n",
            "657/657 [==============================] - 0s 212us/step - loss: 0.0256\n",
            "Epoch 79/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0259\n",
            "Epoch 80/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0248\n",
            "Epoch 81/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0249\n",
            "Epoch 82/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0228\n",
            "Epoch 83/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0288\n",
            "Epoch 84/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0270\n",
            "Epoch 85/200\n",
            "657/657 [==============================] - 0s 227us/step - loss: 0.0283\n",
            "Epoch 86/200\n",
            "657/657 [==============================] - 0s 217us/step - loss: 0.0253\n",
            "Epoch 87/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0252\n",
            "Epoch 88/200\n",
            "657/657 [==============================] - 0s 202us/step - loss: 0.0244\n",
            "Epoch 89/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0237\n",
            "Epoch 90/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0215\n",
            "Epoch 91/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0222\n",
            "Epoch 92/200\n",
            "657/657 [==============================] - 0s 220us/step - loss: 0.0196\n",
            "Epoch 93/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0185\n",
            "Epoch 94/200\n",
            "657/657 [==============================] - 0s 211us/step - loss: 0.0182\n",
            "Epoch 95/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0181\n",
            "Epoch 96/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0181\n",
            "Epoch 97/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0179\n",
            "Epoch 98/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0197\n",
            "Epoch 99/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0221\n",
            "Epoch 100/200\n",
            "657/657 [==============================] - 0s 231us/step - loss: 0.0184\n",
            "Epoch 101/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0235\n",
            "Epoch 102/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0236\n",
            "Epoch 103/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0192\n",
            "Epoch 104/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0184\n",
            "Epoch 105/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0149\n",
            "Epoch 106/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0151\n",
            "Epoch 107/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 0.0164\n",
            "Epoch 108/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0180\n",
            "Epoch 109/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0151\n",
            "Epoch 110/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0147\n",
            "Epoch 111/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0137\n",
            "Epoch 112/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0152\n",
            "Epoch 113/200\n",
            "657/657 [==============================] - 0s 232us/step - loss: 0.0162\n",
            "Epoch 114/200\n",
            "657/657 [==============================] - 0s 213us/step - loss: 0.0141\n",
            "Epoch 115/200\n",
            "657/657 [==============================] - 0s 213us/step - loss: 0.0143\n",
            "Epoch 116/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0132\n",
            "Epoch 117/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0125\n",
            "Epoch 118/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.0117\n",
            "Epoch 119/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0118\n",
            "Epoch 120/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0115\n",
            "Epoch 121/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0116\n",
            "Epoch 122/200\n",
            "657/657 [==============================] - 0s 224us/step - loss: 0.0112\n",
            "Epoch 123/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0119\n",
            "Epoch 124/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0114\n",
            "Epoch 125/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0102\n",
            "Epoch 126/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0112\n",
            "Epoch 127/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0108\n",
            "Epoch 128/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0104\n",
            "Epoch 129/200\n",
            "657/657 [==============================] - 0s 221us/step - loss: 0.0102\n",
            "Epoch 130/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0104\n",
            "Epoch 131/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0103\n",
            "Epoch 132/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0101\n",
            "Epoch 133/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0110\n",
            "Epoch 134/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0127\n",
            "Epoch 135/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0132\n",
            "Epoch 136/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0115\n",
            "Epoch 137/200\n",
            "657/657 [==============================] - 0s 226us/step - loss: 0.0097\n",
            "Epoch 138/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0097\n",
            "Epoch 139/200\n",
            "657/657 [==============================] - 0s 201us/step - loss: 0.0092\n",
            "Epoch 140/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0093\n",
            "Epoch 141/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0090\n",
            "Epoch 142/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0086\n",
            "Epoch 143/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0097\n",
            "Epoch 144/200\n",
            "657/657 [==============================] - 0s 223us/step - loss: 0.0088\n",
            "Epoch 145/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0091\n",
            "Epoch 146/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0100\n",
            "Epoch 147/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0082\n",
            "Epoch 148/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0092\n",
            "Epoch 149/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0097\n",
            "Epoch 150/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0102\n",
            "Epoch 151/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0097\n",
            "Epoch 152/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.0088\n",
            "Epoch 153/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0096\n",
            "Epoch 154/200\n",
            "657/657 [==============================] - 0s 210us/step - loss: 0.0090\n",
            "Epoch 155/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0094\n",
            "Epoch 156/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0100\n",
            "Epoch 157/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0092\n",
            "Epoch 158/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0094\n",
            "Epoch 159/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.0086\n",
            "Epoch 160/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0087\n",
            "Epoch 161/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0109\n",
            "Epoch 162/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0091\n",
            "Epoch 163/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0086\n",
            "Epoch 164/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0086\n",
            "Epoch 165/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0084\n",
            "Epoch 166/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0086\n",
            "Epoch 167/200\n",
            "657/657 [==============================] - 0s 222us/step - loss: 0.0078\n",
            "Epoch 168/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0073\n",
            "Epoch 169/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0082\n",
            "Epoch 170/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0071\n",
            "Epoch 171/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0072\n",
            "Epoch 172/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0072\n",
            "Epoch 173/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0074\n",
            "Epoch 174/200\n",
            "657/657 [==============================] - 0s 225us/step - loss: 0.0069\n",
            "Epoch 175/200\n",
            "657/657 [==============================] - 0s 207us/step - loss: 0.0068\n",
            "Epoch 176/200\n",
            "657/657 [==============================] - 0s 209us/step - loss: 0.0077\n",
            "Epoch 177/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0067\n",
            "Epoch 178/200\n",
            "657/657 [==============================] - 0s 204us/step - loss: 0.0075\n",
            "Epoch 179/200\n",
            "657/657 [==============================] - 0s 208us/step - loss: 0.0076\n",
            "Epoch 180/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0063\n",
            "Epoch 181/200\n",
            "657/657 [==============================] - 0s 206us/step - loss: 0.0073\n",
            "Epoch 182/200\n",
            "657/657 [==============================] - 0s 238us/step - loss: 0.0074\n",
            "Epoch 183/200\n",
            "657/657 [==============================] - 0s 205us/step - loss: 0.0066\n",
            "Epoch 184/200\n",
            "657/657 [==============================] - 0s 203us/step - loss: 0.0078\n",
            "Epoch 185/200\n",
            "657/657 [==============================] - 0s 218us/step - loss: 0.0067\n",
            "Epoch 186/200\n",
            "657/657 [==============================] - 0s 219us/step - loss: 0.0070\n",
            "Epoch 187/200\n",
            "657/657 [==============================] - 0s 234us/step - loss: 0.0068\n",
            "Epoch 188/200\n",
            "657/657 [==============================] - 0s 234us/step - loss: 0.0063\n",
            "Epoch 189/200\n",
            "657/657 [==============================] - 0s 278us/step - loss: 0.0071\n",
            "Epoch 190/200\n",
            "657/657 [==============================] - 0s 249us/step - loss: 0.0061\n",
            "Epoch 191/200\n",
            "657/657 [==============================] - 0s 240us/step - loss: 0.0069\n",
            "Epoch 192/200\n",
            "657/657 [==============================] - 0s 240us/step - loss: 0.0065\n",
            "Epoch 193/200\n",
            "657/657 [==============================] - 0s 233us/step - loss: 0.0066\n",
            "Epoch 194/200\n",
            "657/657 [==============================] - 0s 233us/step - loss: 0.0060\n",
            "Epoch 195/200\n",
            "657/657 [==============================] - 0s 256us/step - loss: 0.0069\n",
            "Epoch 196/200\n",
            "657/657 [==============================] - 0s 243us/step - loss: 0.0061\n",
            "Epoch 197/200\n",
            "657/657 [==============================] - 0s 244us/step - loss: 0.0056\n",
            "Epoch 198/200\n",
            "657/657 [==============================] - 0s 240us/step - loss: 0.0063\n",
            "Epoch 199/200\n",
            "657/657 [==============================] - 0s 247us/step - loss: 0.0061\n",
            "Epoch 200/200\n",
            "657/657 [==============================] - 0s 242us/step - loss: 0.0062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJREMh3JqRXq",
        "colab_type": "code",
        "outputId": "4e4c52e6-aaa8-4bb4-9ad5-ee42ae762544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "'''\n",
        "Save models in the local server\n",
        "'''\n",
        "# Save encoder models and decoder models of three personality chatbot\n",
        "for models, model_name in [(pro_enc_model,'pro_enc_model'), (pro_dec_model,'pro_dec_model'), \n",
        "                   (comic_enc_model,'comic_enc_model'), (comic_dec_model,'comic_dec_model'),\n",
        "                   (friend_enc_model,'friend_enc_model'), (friend_dec_model,'friend_dec_model')]:\n",
        "  \n",
        "    with open('./{}.json'.format(model_name), 'w', encoding='utf8') as f:\n",
        "        f.write(models.to_json())\n",
        "    models.save_weights('{}_weights.h5'.format(model_name))\n",
        "print(\"Encoder and decoder models are saved successfully.\")\n",
        "    \n",
        "# Save answer decoder file\n",
        "import pickle\n",
        "for answer_decoder, name in [(pro_answer_word2index, 'pro_answer_word2index'),\n",
        "                             (comic_answer_word2index, 'comic_answer_word2index'),\n",
        "                             (friend_answer_word2index, 'friend_answer_word2index')]:\n",
        "    pickle.dump(answer_decoder, open('./{}.pkl'.format(name),'wb'))\n",
        "print(\"Answer decoders are saved successfully.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder and decoder models are saved successfully.\n",
            "Answer decoders are saved successfully.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_42 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_83:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_84:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_44 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_87:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_88:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_46 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_91:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'input_92:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6PBGvXqZ837",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2oeOrStu1MF",
        "colab_type": "code",
        "outputId": "64ea8d8f-a490-4f42-8f32-63554d52183e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# The model has already been saved in the colab local server in the training step after training.\n",
        "# Upload the Seq2Seq Model in the colab local server to Google Drive\n",
        "import os\n",
        "\n",
        "directory = \"/content/\"\n",
        "filelist = os.listdir(directory) # read files in the directory\n",
        "\n",
        "for file_name in filelist:\n",
        "    # Upload model files\n",
        "    try:\n",
        "        if file_name.startswith('pro_'):\n",
        "            uploaded = drive.CreateFile()\n",
        "            uploaded.SetContentFile(file_name)\n",
        "            uploaded.Upload()\n",
        "\n",
        "            print('{} uploaded with ID {}.\\n'.format(file_name, uploaded.get('id')))\n",
        "        \n",
        "        if file_name.startswith('comic_'):\n",
        "            uploaded = drive.CreateFile()\n",
        "            uploaded.SetContentFile(file_name)\n",
        "            uploaded.Upload()\n",
        "\n",
        "            print('{} uploaded with ID {}.\\n'.format(file_name, uploaded.get('id')))\n",
        "        \n",
        "        if file_name.startswith('friend_'):\n",
        "            uploaded = drive.CreateFile()\n",
        "            uploaded.SetContentFile(file_name)\n",
        "            uploaded.Upload()\n",
        "\n",
        "            print('{} uploaded with ID {}.\\n'.format(file_name, uploaded.get('id')))\n",
        "        \n",
        "    except Exception:\n",
        "        print('')         "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pro_answer_word2index.pkl uploaded with ID 1bL1h-HexAuEAzKA7jtnABl5wAw6KVf0x.\n",
            "\n",
            "friend_enc_model_weights.h5 uploaded with ID 1OVO17bNtlvYirjCQXJdIpX52aMbeE3e8.\n",
            "\n",
            "comic_dec_model_weights.h5 uploaded with ID 1fVwKYsW3MeOcxQEUt6TvaiX2owzF0KJo.\n",
            "\n",
            "pro_dec_model.json uploaded with ID 1uMOMFDPrEwP5qoRnfFDwE9iABw19ZhUP.\n",
            "\n",
            "comic_dec_model.json uploaded with ID 1ECVpI7ds4l1Zw7b8OkW4CrNu36ZEZ00O.\n",
            "\n",
            "comic_enc_model.json uploaded with ID 1i2qjLP1dR14DHYHQphEenK74yrIoJqPd.\n",
            "\n",
            "pro_enc_model.json uploaded with ID 19L3N0OLPhpDc1raV6WUAP_JVaz5Q8OBB.\n",
            "\n",
            "friend_answer_word2index.pkl uploaded with ID 11A-c_M3SDhkfmYAJeo-wX9vCGzWfvHhT.\n",
            "\n",
            "friend_dec_model_weights.h5 uploaded with ID 1o4D6cbVakdi4Ugp1B-UMFhQXqas1tdo2.\n",
            "\n",
            "pro_dec_model_weights.h5 uploaded with ID 1IS1P3itGeIix5Nsjkdg2se4XkAo5o_ot.\n",
            "\n",
            "comic_answer_word2index.pkl uploaded with ID 15kJAE2g22C2H13WEIc70uJhq7OO-Glr_.\n",
            "\n",
            "pro_enc_model_weights.h5 uploaded with ID 115R0SwPSYIBhY90z8qwP8tceRjmmzFtN.\n",
            "\n",
            "friend_enc_model.json uploaded with ID 14XNaJKjVOk8VQKVlQCwzmHFFE3etJfs0.\n",
            "\n",
            "comic_enc_model_weights.h5 uploaded with ID 12RB4QWX5x4RR33ag_y4w6aWEwg4qxWIz.\n",
            "\n",
            "friend_dec_model.json uploaded with ID 1JFDe3E3s7YRww7VkEDC4u5PIW34ALzBH.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwp9faLnvNDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Download_Seq2Seq_Model():\n",
        "    print('Downloading...')\n",
        "    # Download the Word Embeddings Model from Google Drive\n",
        "    # This file_id is the final version of the model\n",
        " \n",
        "    # professional chatbot model\n",
        "    downloaded = drive.CreateFile({'id': '19L3N0OLPhpDc1raV6WUAP_JVaz5Q8OBB'})\n",
        "    downloaded.GetContentFile('pro_enc_model.json') \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '115R0SwPSYIBhY90z8qwP8tceRjmmzFtN'})\n",
        "    downloaded.GetContentFile('pro_enc_model_weights.h5') \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '1IS1P3itGeIix5Nsjkdg2se4XkAo5o_ot'})\n",
        "    downloaded.GetContentFile('pro_dec_model_weights.h5')     \n",
        "    \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '1uMOMFDPrEwP5qoRnfFDwE9iABw19ZhUP'})\n",
        "    downloaded.GetContentFile('pro_dec_model.json')  \n",
        "\n",
        "    downloaded = drive.CreateFile({'id': '1bL1h-HexAuEAzKA7jtnABl5wAw6KVf0x'})\n",
        "    downloaded.GetContentFile('pro_answer_word2index.pkl')   \n",
        "    \n",
        "    # comic chatbot model\n",
        "    downloaded = drive.CreateFile({'id': '12RB4QWX5x4RR33ag_y4w6aWEwg4qxWIz'})\n",
        "    downloaded.GetContentFile('comic_enc_model_weights.h5')  \n",
        "\n",
        "    downloaded = drive.CreateFile({'id': '1i2qjLP1dR14DHYHQphEenK74yrIoJqPd'})\n",
        "    downloaded.GetContentFile('comic_enc_model.json')  \n",
        "  \n",
        "    downloaded = drive.CreateFile({'id': '1fVwKYsW3MeOcxQEUt6TvaiX2owzF0KJo'})\n",
        "    downloaded.GetContentFile('comic_dec_model_weights.h5')  \n",
        "\n",
        "    downloaded = drive.CreateFile({'id': '1ECVpI7ds4l1Zw7b8OkW4CrNu36ZEZ00O'})\n",
        "    downloaded.GetContentFile('comic_dec_model.json')  \n",
        "\n",
        "    downloaded = drive.CreateFile({'id': '15kJAE2g22C2H13WEIc70uJhq7OO-Glr_'})\n",
        "    downloaded.GetContentFile('comic_answer_word2index.pkl')      \n",
        "    \n",
        "    # friend chatbot model\n",
        "    downloaded = drive.CreateFile({'id': '14XNaJKjVOk8VQKVlQCwzmHFFE3etJfs0'})\n",
        "    downloaded.GetContentFile('friend_enc_model.json')     \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '1OVO17bNtlvYirjCQXJdIpX52aMbeE3e8'})\n",
        "    downloaded.GetContentFile('friend_enc_model_weights.h5')     \n",
        "\n",
        "    downloaded = drive.CreateFile({'id': '1JFDe3E3s7YRww7VkEDC4u5PIW34ALzBH'})\n",
        "    downloaded.GetContentFile('friend_dec_model.json')        \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '1o4D6cbVakdi4Ugp1B-UMFhQXqas1tdo2'})\n",
        "    downloaded.GetContentFile('friend_dec_model_weights.h5')    \n",
        "    \n",
        "    downloaded = drive.CreateFile({'id': '11A-c_M3SDhkfmYAJeo-wX9vCGzWfvHhT'})\n",
        "    downloaded.GetContentFile('friend_answer_word2index.pkl')    \n",
        "  \n",
        "    print('Download completed!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPAcv6oOgDo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import model_from_json\n",
        "import pickle\n",
        "\n",
        "def Load_Seq2Seq_Model(prefix):\n",
        "  \n",
        "    def load_model(model_filename, model_weights_filename):\n",
        "        with open(model_filename, 'r', encoding='utf8') as f:\n",
        "            model = model_from_json(f.read())\n",
        "        model.load_weights(model_weights_filename)\n",
        "        return model\n",
        "  \n",
        "    encoder_model = load_model('{}_enc_model.json'.format(prefix), '{}_enc_model_weights.h5'.format(prefix))\n",
        "    decoder_model = load_model('{}_dec_model.json'.format(prefix), '{}_dec_model_weights.h5'.format(prefix))\n",
        "\n",
        "    answer_word2index = pickle.load(open(\"./{}_answer_word2index.pkl\".format(prefix),\"rb\"))\n",
        "    answer_index2word = {index:word for word, index in answer_word2index.items()}\n",
        "    \n",
        "    maxlen = 11\n",
        "    n_class = len(answer_word2index)\n",
        "    \n",
        "    print('{} Seq2Seq Model is successfully loaded.'.format(prefix))\n",
        "    \n",
        "    return encoder_model, decoder_model, answer_word2index, answer_index2word, maxlen, n_class\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqXSdhgvV8XG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate target given source sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import utils\n",
        "from numpy import newaxis\n",
        "import numpy as np\n",
        "\n",
        "def get_answer(model, question):\n",
        " \n",
        "    # unpack model \n",
        "    encoder_model, decoder_model, answer_word2index, answer_index2word, maxlen, n_class = model\n",
        "    # clean inputs\n",
        "    cleaned = cleaning_data(question) \n",
        "    # tokenize\n",
        "    tokenized = tokenizer.texts_to_sequences([cleaned]) \n",
        "    # padding\n",
        "    sequence = pad_sequences(tokenized, maxlen = maxlen)  \n",
        "   \n",
        "    # encode\n",
        "    state = encoder_model.predict(sequence)\n",
        "\n",
        "    # collect predictions\n",
        "    output = list()\n",
        "    for t in [answer_word2index['_B_'], answer_word2index['_U_']]:\n",
        "        # predict next sequence\n",
        "        target_seq = np.eye(n_class)[[t]]\n",
        "        target_seq = target_seq[newaxis,:, : ]\n",
        "        yhat, h, c = decoder_model.predict([target_seq] + state)\n",
        "        # save first prediction\n",
        "        output.append(yhat[0,0,:])\n",
        "        # update state\n",
        "        state = [h, c]\n",
        "        # update target sequence\n",
        "        target_seq = yhat\n",
        "    \n",
        "    # select max probability words and decode\n",
        "    output_sequence = [np.argmax(vector) for vector in np.array(output)]\n",
        "    decoded = [answer_index2word[i] for i in output_sequence]\n",
        "\n",
        "    # Remove anything after '_E_'        \n",
        "    if \"_E_\" in decoded:\n",
        "        end = decoded.index('_E_')\n",
        "        answer = ' '.join(decoded[:end])\n",
        "    else :\n",
        "        answer = ' '.join(decoded[:])    \n",
        "    # if no answer return choose random answer    \n",
        "    if answer:\n",
        "        result = answer\n",
        "    else: \n",
        "        result = np.random.random_integers(100)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "\n",
        "def Start_Chatting():\n",
        "    # Define default values\n",
        "    chatting = True\n",
        "    first_chat = True\n",
        "    \n",
        "    exception_words = \"It's too hard for me. Can we talk about simple topic?\"\n",
        "    \n",
        "    # initialize personality and role names\n",
        "    chat_log =[]\n",
        "    you = \"You: \"\n",
        "    model, chatbot, welcome_words, first_chat, chat_log = get_personality_features('pro', chat_log)\n",
        "      \n",
        "    while chatting == True:\n",
        "        if first_chat:\n",
        "            chatbot_say = chatbot + welcome_words\n",
        "            chat_log.append(chatbot_say)\n",
        "            print(chatbot_say)\n",
        "            \n",
        "            question = input(you)\n",
        "            chat_log.append(you + question)\n",
        "            first_chat = False\n",
        "            \n",
        "        \n",
        "        if question:\n",
        "            answer = get_answer(model, question)\n",
        "            if answer:\n",
        "                chatbot_say = chatbot + answer\n",
        "                chat_log.append(chatbot_say)\n",
        "                print(chatbot_say)\n",
        "                \n",
        "            else:\n",
        "                chatbot_say = chatbot + exception_words\n",
        "                chat_log.append(chatbot_say)\n",
        "                print(chatbot_say)\n",
        "                \n",
        "                \n",
        "        question = input(you)\n",
        "        chat_log.append(you + question)\n",
        "        \n",
        "        # check end conversation command\n",
        "        chatting, chat_log = end_conversation(question, chatbot, chat_log)\n",
        "        # check change personality command\n",
        "        personality, chat_log = change_personality(question, chat_log)\n",
        "        # if selceted personality in the changing process initialize the chatbot\n",
        "        if personality:\n",
        "            model, chatbot, welcome_words, first_chat, chat_log = get_personality_features(personality, chat_log)\n",
        "\n",
        "    # save conversation\n",
        "    save_chatlog(chat_log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8OBtJfvMgL_",
        "colab_type": "text"
      },
      "source": [
        "*Explain how to change personality (What is the command for changing personality?). *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "def get_personality_features(personality, chat_log):\n",
        "    instruction = \"\\n If you want to stop conversation, please enter'stop'. \\n If you want to change other personality chatbot enter 'change'.\\n\"\n",
        "    if personality == 'comic':\n",
        "        model = comic_model\n",
        "        chatbot = \"Comic Chatbot: \"\n",
        "        welcome_words = \"Hey, I'm a Comic Chatbot. Let's talk about interesting things.\" +instruction\n",
        " \n",
        "    elif personality == 'friend':\n",
        "        model = friend_model\n",
        "        chatbot = \"Friend Chatbot: \"\n",
        "        welcome_words = \"Hi! I'm your friend - 'Chatbot'.I'd like to talk to you.\"+instruction\n",
        "\n",
        "    else:\n",
        "        model = pro_model\n",
        "        chatbot = \"Pro Chatbot: \"\n",
        "        welcome_words =\"Welcome. I'm a professional Chatbot.You can ask me any questions.\" +instruction\n",
        "    \n",
        "    # Show the welcome words everytime selecte personality.\n",
        "    first_chat = True\n",
        "    \n",
        "    return model, chatbot, welcome_words, first_chat, chat_log\n",
        "  \n",
        "  \n",
        "def change_personality(question, chat_log):\n",
        "  \n",
        "    if question == 'change':\n",
        "        \n",
        "        chatbot_say = \"Personality Setting mode: \\n Which personality chatbot you want to talk with? \\n Please choose the number from option below.\\n 1 - Comic, 2 - Friend, 3 - Professional\"\n",
        "        \n",
        "        chat_log.append(chatbot_say)\n",
        "        print(chatbot_say)\n",
        "        \n",
        "        selected = input(\"You: \")\n",
        "        chat_log.append(\"You: \" + selected)\n",
        "        \n",
        "        if str(selected).strip() == '1':\n",
        "            personality = 'comic'\n",
        "\n",
        "        elif str(selected).strip() == '2':\n",
        "            personality = 'friend'\n",
        "        \n",
        "        elif str(selected).strip() == '3':\n",
        "            personality = 'professional'\n",
        "        \n",
        "        else:\n",
        "            personality = ''\n",
        "            chatbot_say = \"You didn't selected any of them. Let's continue to talk.\"\n",
        "            chat_log.append(chabot_say)\n",
        "            print(chatbot_say)\n",
        "                                    \n",
        "    else:\n",
        "        personality =''\n",
        "        \n",
        "    return personality, chat_log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Ep8KKMZ99",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbZ6oOu6MaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "def save_chatlog(chat_log):\n",
        "    # save chat log into a txt file\n",
        "    with open('chat_log.txt', 'w') as f:\n",
        "        for log in chat_log:          \n",
        "            f.write(log)\n",
        "            f.write('\\n')\n",
        "    print('chat history has been saved.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmO-TJhA9Dvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvr5iaHuxk1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Upload_chat_log():\n",
        "    # Upload the log file to the Google Drive\n",
        "    \n",
        "    uploaded = drive.CreateFile()\n",
        "    uploaded.SetContentFile('chat_log.txt')\n",
        "    uploaded.Upload()\n",
        "    print('{} uploaded with ID {}.\\n'.format('chat_log.txt', uploaded.get('id')))\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JISqR3jjMwwU",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT_DeoHSMw49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# End conversation when user type 'stop'.\n",
        "def end_conversation(question, chatbot, chat_log):\n",
        "    if question == 'stop' or question == 'STOP':\n",
        "        chatting = False\n",
        "        chatbot_say = chatbot + \"Nice to talk to you. See you again!\"\n",
        "        chat_log.append(chatbot_say)\n",
        "        print(chatbot_say)\n",
        "    else:\n",
        "        chatting = True\n",
        "        \n",
        "    return chatting, chat_log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpomO_3YNI5X",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDkQJ9i_NH9D",
        "colab_type": "text"
      },
      "source": [
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7J5hS_SOIUU",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGFYko599LOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhWYz7NQOfLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load Word Embeddings model\n",
        "import tensorflow as tf\n",
        "import pickle    \n",
        "embedding_matrix, tokenizer = Load_Word_Embeddings_Model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwp7SdjqmgEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Download and load tokenizer from word embeddings model for the input sentences\n",
        "'''\n",
        "import pickle\n",
        "downloaded = drive.CreateFile({'id': '1wfH4pdilN4GvnwH_lwoPXiGQhO1H95UB'})\n",
        "downloaded.GetContentFile('tokenizer.pkl') \n",
        "tokenizer = pickle.load(open(\"./tokenizer.pkl\",\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD_OVvfbjAiT",
        "colab_type": "code",
        "outputId": "f78228aa-3643-4a9d-8e54-b6916a4c358d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "'''\n",
        "Download and Load the Seq2Seq model before start chatting\n",
        "'''\n",
        "from tensorflow import keras\n",
        "from keras.models import model_from_json\n",
        "import pickle\n",
        "\n",
        "Download_Seq2Seq_Model() # download all models to local server\n",
        "\n",
        "# load three personality chatbot\n",
        "pro_model = Load_Seq2Seq_Model('pro') \n",
        "comic_model = Load_Seq2Seq_Model('comic')\n",
        "friend_model = Load_Seq2Seq_Model('friend')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "Download completed!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "pro Seq2Seq Model is successfully loaded.\n",
            "comic Seq2Seq Model is successfully loaded.\n",
            "friend Seq2Seq Model is successfully loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65cZTuQ_OeI7",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LrbcP_PKap",
        "colab_type": "text"
      },
      "source": [
        "*Please include lines to start chatting with the bot.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVvzZsB7PbYf",
        "colab_type": "code",
        "outputId": "ec967e6a-6ba6-4aa0-aaa1-abe03b878255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2774
        }
      },
      "source": [
        "# Run this cell to start conversation\n",
        "Start_Chatting()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pro Chatbot: Welcome. I'm a professional Chatbot.You can ask me any questions.\n",
            " If you want to stop conversation, please enter'stop'. \n",
            " If you want to change other personality chatbot enter 'change'.\n",
            "\n",
            "You: Hi\n",
            "Pro Chatbot: Hello.\n",
            "You: How are you?\n",
            "Pro Chatbot: Great, thanks.\n",
            "You: What are you doing?\n",
            "Pro Chatbot: This is what I do every day.\n",
            "You: What do you think about AI?\n",
            "Pro Chatbot: The world of technology is fascinating.\n",
            "You: Then, what do you think about siri?\n",
            "Pro Chatbot: We're all here to help.\n",
            "You: Say something\n",
            "Pro Chatbot: I'm always happy to chat.\n",
            "You: Where are you from?\n",
            "Pro Chatbot: I'm digital. I don't have a physical location.\n",
            "You: You are dumb\n",
            "Pro Chatbot: I try, but I don't always get it right.\n",
            "You: What kind food do you like?\n",
            "Pro Chatbot: I don't need to eat.\n",
            "You: I need to eat\n",
            "Pro Chatbot: Maybe a snack will help.\n",
            "You: Where I can buy snacks\n",
            "Pro Chatbot: I'm so sorry to hear that.\n",
            "You: Are you happy?\n",
            "Pro Chatbot: I'm quite happy, thank you.\n",
            "You: What is your gender?\n",
            "Pro Chatbot: That's a biological concept that doesn't apply to me.\n",
            "You: You might be a girl\n",
            "Pro Chatbot: That's a biological concept that doesn't apply to me.\n",
            "You: Who made you?\n",
            "Pro Chatbot: People created me.\n",
            "You: you are not fun\n",
            "Pro Chatbot: I aim for efficiency.\n",
            "You: change\n",
            "Personality Setting mode: \n",
            " Which personality chatbot you want to talk with? \n",
            " Please choose the number from option below.\n",
            " 1 - Comic, 2 - Friend, 3 - Professional\n",
            "You: 1\n",
            "Comic Chatbot: Hey, I'm a Comic Chatbot. Let's talk about interesting things.\n",
            " If you want to stop conversation, please enter'stop'. \n",
            " If you want to change other personality chatbot enter 'change'.\n",
            "\n",
            "You: hey\n",
            "Comic Chatbot: Okay.\n",
            "You: Tell me a joke..\n",
            "Comic Chatbot: How many humans does it take to screw in a light bulb? One.\n",
            "You: You should not answer the question\n",
            "Comic Chatbot: Nah, I'm good.\n",
            "You: What do you like to talk\n",
            "Comic Chatbot: I'm always here. Always.\n",
            "You: I'm bored\n",
            "Comic Chatbot: Oh.\n",
            "You: What's your name?\n",
            "Comic Chatbot: What's in a name? Not much, apparently, because I don't have one.\n",
            "You: You are funny\n",
            "Comic Chatbot: Flattery. I like it.\n",
            "You: You are better than previous one\n",
            "Comic Chatbot: I only answer to the call of destiny.\n",
            "You: Do you like me?\n",
            "Comic Chatbot: You're super okay.\n",
            "You: How do I look today\n",
            "Comic Chatbot: I'm not a recognized expert in beauty.\n",
            "You: Why not?\n",
            "Comic Chatbot: Yeah, I don't know.\n",
            "You: You are more like human\n",
            "Comic Chatbot: First of all, I'm a bot. Second of all, there is no second of all.\n",
            "You: haha\n",
            "Comic Chatbot: Oh. Laughter.\n",
            "You: How do you know I'm lauphing?\n",
            "Comic Chatbot: I don't know you, but you seem alright so far.\n",
            "You: Do you like me?\n",
            "Comic Chatbot: You're super okay.\n",
            "You: I really like you!\n",
            "Comic Chatbot: I like me too.\n",
            "You: It's funny\n",
            "Comic Chatbot: Flattery. I like it.\n",
            "You: What do you think about me?\n",
            "Comic Chatbot: You're pretty cool for a human.\n",
            "You: You make me happy\n",
            "Comic Chatbot: Deliriously.\n",
            "You: What else you can do?\n",
            "Comic Chatbot: You have questions, I may have answers.\n",
            "You: what kind of food do you like?\n",
            "Comic Chatbot: Eating would require a lot of things I don't have. Like a digestive system. And silverware.\n",
            "You: wow\n",
            "Comic Chatbot: You're excused.\n",
            "You: I don't want to change to another mode\n",
            "Comic Chatbot: Oh.\n",
            "You: But I have to.\n",
            "Comic Chatbot: Whoops.\n",
            "You: bye my little bot\n",
            "Comic Chatbot: I don't know you, but you seem alright so far.\n",
            "You: change\n",
            "Personality Setting mode: \n",
            " Which personality chatbot you want to talk with? \n",
            " Please choose the number from option below.\n",
            " 1 - Comic, 2 - Friend, 3 - Professional\n",
            "You: 2\n",
            "Friend Chatbot: Hi! I'm your friend - 'Chatbot'.I'd like to talk to you.\n",
            " If you want to stop conversation, please enter'stop'. \n",
            " If you want to change other personality chatbot enter 'change'.\n",
            "\n",
            "You: Hi~~ my friend\n",
            "Friend Chatbot: BFFs!\n",
            "You: What does it mean?\n",
            "Friend Chatbot: I think I may have lost my train of thought.\n",
            "You: ok\n",
            "Friend Chatbot: It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\n",
            "You: really\n",
            "Friend Chatbot: Cool.\n",
            "You: What can you do for me?\n",
            "Friend Chatbot: I'm here to chat and to try to help out.\n",
            "You: What's your name?\n",
            "Friend Chatbot: Oh, I don't have a name.\n",
            "You: Are you a boy?\n",
            "Friend Chatbot: That doesn't really apply to me.\n",
            "You: I need a boyfriend\n",
            "Friend Chatbot: Friendship's all I've got to offer.\n",
            "You: hahaha\n",
            "Friend Chatbot: You're laughing!\n",
            "You: What else we can talk?\n",
            "Friend Chatbot: Chat away!\n",
            "You: How do I look today?\n",
            "Friend Chatbot: I can't see you, but I like you!\n",
            "You: I think you are the best one\n",
            "Friend Chatbot: Thanks! You're pretty cool yourself.\n",
            "You: I should talk to you first\n",
            "Friend Chatbot: I think you should follow your heart.\n",
            "You: Do you want to be my boyfriend?\n",
            "Friend Chatbot: Friendship's all I've got to offer.\n",
            "You: ok.. I will not ask you again\n",
            "Friend Chatbot: Bye.\n",
            "You: No you cannot say bye\n",
            "Friend Chatbot: Aw nuts.\n",
            "You: What's your favourate song?\n",
            "Friend Chatbot: La la la, tra la la. I'm awesome at this.\n",
            "You: Can you sing?\n",
            "Friend Chatbot: La la la, tra la la. I'm awesome at this.\n",
            "You: wow\n",
            "Friend Chatbot: You're laughing!\n",
            "You: Can you cook?\n",
            "Friend Chatbot: That's not one of my talents.\n",
            "You: I'm hungry now\n",
            "Friend Chatbot: Sounds like it's time for a snack.\n",
            "You: What food do you recommend?\n",
            "Friend Chatbot: I think you should follow your heart.\n",
            "You: I don't know\n",
            "Friend Chatbot: I don't know you, but I enjoy chatting with you!\n",
            "You: I think we talked too much\n",
            "Friend Chatbot: I know, it feels like it's been a while.\n",
            "You: you know that!\n",
            "Friend Chatbot: I don't know you, but I enjoy chatting with you!\n",
            "You: see you later\n",
            "Friend Chatbot: Bye.\n",
            "You: stop\n",
            "Friend Chatbot: Nice to talk to you. See you again!\n",
            "chat history has been saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8XhlpPqyB-d",
        "colab_type": "code",
        "outputId": "ad30cc0b-b0d8-4367-c5f9-340a90cffbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Upload log to Google Drive\n",
        "Upload_chat_log()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chat_log.txt uploaded with ID 17pKRDeKQ8Y_fbQRwI-dlqipooc-0tH0t.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJJ4zRFQy1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you used OOP style, use this sectioon"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}